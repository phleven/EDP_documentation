from pyspark.sql.types import (
    StructType, StructField, IntegerType, StringType, DateType
)
from pyspark.sql.functions import col, lit, current_timestamp
from delta.tables import DeltaTable

target_delta_path = "/mnt/delta/dim_customer"
primary_key = "Taxpayer"

schema = StructType([
    StructField("Taxpayer", IntegerType(), True),
    StructField("First_Name", StringType(), True),
    StructField("Last_Name", StringType(), True),
    StructField("Filing_Status", StringType(), True),
    StructField("Adjusted_Gross_Income", IntegerType(), True),
    StructField("Tax_Year", IntegerType(), True),
    StructField("Address", StringType(), True),
    StructField("Email", StringType(), True),
    StructField("Update_Timestamp", DateType(), True),
    StructField("Deleted", StringType(), True)    
])

source_df = (
    spark.read.format("csv")
    .option("header", True)
    .schema(schema)
    .load("dbfs:/Volumes/irs_group_catalog/lakeflow_demo_schema/landing_zone/01-02-26_1040.csv")
)

source_df = (
    source_df
    .withColumn("start_date", current_timestamp())
    .withColumn("end_date", lit(None).cast("timestamp"))
    .withColumn("is_current", lit(True))
)

if DeltaTable.isDeltaTable(spark, target_delta_path):
    delta_table = DeltaTable.forPath(spark, target_delta_path)
    delta_table.alias("tgt").merge(
        source_df.alias("src"),
        f"tgt.{primary_key} = src.{primary_key} AND tgt.is_current = true"
    ).whenMatchedUpdate(
        condition=" OR ".join([
            f"tgt.{c} <> src.{c}" for c in source_df.columns if c not in ["start_date", "end_date", "is_current"]
        ]),
        set={
            "end_date": current_timestamp(),
            "is_current": lit(False)
        }
    ).whenNotMatchedInsertAll().execute()
else:
    source_df.write.format("delta").mode("overwrite").save(target_delta_path)

final_df = spark.read.format("delta").load(target_delta_path)
display(final_df)